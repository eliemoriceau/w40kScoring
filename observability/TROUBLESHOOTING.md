# Guide de Troubleshooting - Infrastructure d'ObservabilitÃ© W40K

Guide complet de rÃ©solution des problÃ¨mes courants de la stack d'observabilitÃ©.

## ğŸš¨ ProblÃ¨mes critiques rÃ©solus

### âŒ Loki en crash loop constant

**SymptÃ´mes** :

- Container status : `Restarting (137)`
- Logs : `"empty ring"`, `"error getting ingester clients"`
- Grafana : `"dial tcp: lookup loki: no such host"`

**Diagnostic** :

```bash
docker ps | grep loki              # Status Restarting
docker logs w40k-loki --tail 10   # Erreurs "empty ring"
```

**Cause racine identifiÃ©e** :
Loki 3.4.2 force structured metadata par dÃ©faut, incompatible avec schema v11

**Solution appliquÃ©e** :

```yaml
# observability/loki.yml
limits_config:
  allow_structured_metadata: false # CRITIQUE
```

**Validation** :

```bash
docker ps | grep loki  # Status "Up X seconds"
docker logs w40k-loki --tail 5  # Pas d'erreur "empty ring"
```

---

### âŒ Grafana ne trouve pas Loki (DNS lookup failed)

**SymptÃ´mes** :

- Error : `dial tcp: lookup loki on 127.0.0.11:53: no such host`
- Dashboard logs vides
- Test connection failed dans Grafana

**Diagnostic** :

```bash
docker ps | grep loki                    # VÃ©rifier statut Loki
docker logs w40k-loki                   # Analyser crash loops
docker exec w40k-grafana nslookup loki  # Test rÃ©solution DNS
```

**Cause racine identifiÃ©e** :
Loki inaccessible Ã  cause du crash loop, pas un problÃ¨me DNS

**Solution** :

1. Corriger configuration Loki (voir section prÃ©cÃ©dente)
2. RedÃ©marrer Loki : `docker compose -f docker-compose.observability.yml restart loki`
3. Attendre stabilisation (30s)
4. Tester depuis Grafana

---

### âŒ Logs non remontÃ©s dans Loki/Grafana

**SymptÃ´mes** :

- Dashboards logs vides
- Loki query : `total_entries=0`
- Promtail fonctionne mais pas de donnÃ©es

**Diagnostic** :

```bash
curl -s "http://localhost:3100/loki/api/v1/query_range?query={job=\"w40k-app\"}"
docker logs w40k-promtail --tail 10
```

**Cause racine identifiÃ©e** :
Application W40K tourne sur l'hÃ´te (npm run dev), pas en container Docker. Promtail configurÃ© pour Docker seulement.

**Solutions testÃ©es** :

1. âœ… **Fichier temporaire** : Logs â†’ `/tmp/w40k-app.log` â†’ Promtail
2. âœ… **OpenTelemetry SDK** : Envoi direct OTLP â†’ Collector â†’ Loki
3. âœ… **Docker service discovery** : Collection containers Docker

**Configuration finale** :

```yaml
# promtail.yml - Collection Docker
scrape_configs:
  - job_name: w40k-docker-logs
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
```

---

## ğŸ”§ Diagnostics par service

### Prometheus

**ProblÃ¨mes courants** :

- Targets down/unreachable
- Scraping errors
- Storage full

**Diagnostic** :

```bash
curl http://localhost:9090/api/v1/targets  # VÃ©rifier targets
docker logs w40k-prometheus --tail 10     # Erreurs scraping
docker exec w40k-prometheus df -h         # Usage disque
```

**Solutions** :

- Target down : VÃ©rifier app sur port 3333
- Storage : RÃ©duire retention ou nettoyer donnÃ©es
- Scraping : Corriger configuration dans `prometheus.yml`

### Grafana

**ProblÃ¨mes courants** :

- Datasources unreachable
- Dashboard import failed
- Permission errors

**Diagnostic** :

```bash
docker logs w40k-grafana --tail 15
curl http://localhost:3000/api/health
docker exec w40k-grafana ls /var/lib/grafana/dashboards
```

**Solutions** :

- Datasource : Tester connectivitÃ© depuis Grafana UI
- Dashboard : VÃ©rifier JSON validity
- Permissions : VÃ©rifier volumes et ownership

### OpenTelemetry Collector

**ProblÃ¨mes courants** :

- Receiver errors (OTLP)
- Processor crashes
- Exporter timeouts

**Diagnostic** :

```bash
docker logs w40k-otel-collector --tail 20
curl http://localhost:4318/v1/logs  # Test OTLP endpoint
docker exec w40k-otel-collector cat /etc/otelcol-contrib/otel-collector.yml
```

**Solutions** :

- Config : Valider YAML syntax
- Memory : Ajuster memory_limiter processor
- Network : VÃ©rifier connectivity vers Loki/Tempo

---

## ğŸ“Š Commandes de diagnostic essentielles

### VÃ©rification globale

```bash
# Status tous services
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

# Health check automatique
for svc in grafana loki prometheus; do
  echo "=== $svc ==="
  docker logs w40k-$svc --tail 3
  echo ""
done

# VÃ©rification rÃ©seau
docker network inspect w40kscoring_observability
```

### Tests de connectivitÃ©

```bash
# Endpoints de santÃ©
curl -s http://localhost:3100/ready           # Loki
curl -s http://localhost:9090/-/ready         # Prometheus
curl -s http://localhost:3000/api/health      # Grafana

# Test mÃ©triques application
curl -s http://localhost:3333/metrics | head -10

# Test logs query
curl -G "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query={job="w40k-docker-logs"}'
```

### Debug avancÃ©

```bash
# Logs avec timestamps
docker logs w40k-loki --timestamps --tail 50

# Stats containers
docker stats w40k-grafana w40k-loki w40k-prometheus --no-stream

# Inspect volumes
docker volume ls | grep w40k
docker volume inspect w40kscoring_grafana-data
```

---

## ğŸ› ï¸ ProcÃ©dures de rÃ©paration

### RedÃ©marrage ordonnÃ©

```bash
# 1. ArrÃªt en ordre inverse
docker compose -f docker-compose.observability.yml stop grafana
docker compose -f docker-compose.observability.yml stop prometheus
docker compose -f docker-compose.observability.yml stop loki

# 2. Nettoyage si nÃ©cessaire
docker compose -f docker-compose.observability.yml down

# 3. RedÃ©marrage
docker compose -f docker-compose.observability.yml up -d

# 4. VÃ©rification
sleep 30
docker ps | grep -E "(grafana|loki|prometheus)"
```

### Reset complet (dernier recours)

```bash
# ATTENTION : Perte de toutes les donnÃ©es
docker compose -f docker-compose.observability.yml down -v
docker volume prune -f
docker compose -f docker-compose.observability.yml up -d
```

### RÃ©cupÃ©ration de configuration

```bash
# Backup avant intervention
tar czf observability-backup-$(date +%Y%m%d_%H%M).tar.gz observability/

# Restore configuration
tar xzf observability-backup-YYYYMMDD_HHMM.tar.gz
docker compose -f docker-compose.observability.yml restart
```

---

## ğŸ“ˆ Monitoring de l'infrastructure

### MÃ©triques systÃ¨me importantes

```promql
# Containers up/down
up{job="prometheus"}
up{job="node-exporter"}

# Usage ressources
container_memory_usage_bytes{name=~"w40k-.*"}
container_cpu_usage_seconds_total{name=~"w40k-.*"}

# SantÃ© Loki
loki_ingester_chunks_created_total
loki_distributor_ingester_appends_total

# Performance Prometheus
prometheus_rule_evaluation_duration_seconds
prometheus_tsdb_compactions_total
```

### Alertes recommandÃ©es (futur)

```yaml
# Container down
- alert: ContainerDown
  expr: up{job=~"w40k-.*"} == 0

# High memory usage
- alert: HighMemoryUsage
  expr: container_memory_usage_bytes{name=~"w40k-.*"} > 500MB

# Loki ingestion errors
- alert: LokiIngestionErrors
  expr: increase(loki_distributor_ingester_append_failures_total[5m]) > 10
```

---

## ğŸ¯ Checklist de rÃ©solution

Avant d'escalader un problÃ¨me :

- [ ] Services status checked (`docker ps`)
- [ ] Recent logs analyzed (`docker logs --tail 20`)
- [ ] Network connectivity tested (`curl` health endpoints)
- [ ] Configuration validated (YAML syntax)
- [ ] Resources checked (CPU, memory, disk)
- [ ] Volumes permissions verified
- [ ] Recent changes identified (config, data, network)

**Pour support avancÃ©** : Inclure outputs des commandes ci-dessus + description timeline du problÃ¨me.

---

**Guide maintenu par l'Ã©quipe DevOps W40K** ğŸš€
