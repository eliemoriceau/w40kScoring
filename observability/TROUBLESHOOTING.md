# Guide de Troubleshooting - Infrastructure d'Observabilit√© W40K

Guide complet de r√©solution des probl√®mes critiques identifi√©s et r√©solus lors de l'impl√©mentation Phase 4C.

## üö® Probl√®mes Critiques R√©solus

### ‚ùå Probl√®me #1: Loki en Crash Loop Constant

**Sympt√¥mes Observ√©s**:
- Container status: `Restarting (137)` en boucle continue
- Logs Loki: `"empty ring"`, `"error getting ingester clients"`  
- Grafana: `"dial tcp: lookup loki: no such host"`
- Impossibilit√© d'acc√©der √† l'API Loki sur port 3100

**Diagnostic Effectu√©**:
```bash
# V√©rification status container
docker ps | grep loki              # Status Restarting
docker logs w40k-loki --tail 10   # Erreurs "empty ring"

# Test connectivit√©
curl http://localhost:3100/ready  # Connection refused

# Inspection configuration
docker exec w40k-loki cat /etc/loki/local-config.yaml
```

**Cause Racine Identifi√©e**:
Loki v3.4.2 force `structured_metadata: true` par d√©faut, incompatible avec schema v11 utilis√©.

**Solution Appliqu√©e**:
```yaml
# observability/loki.yml - Configuration critique ajout√©e
limits_config:
  allow_structured_metadata: false  # CRITIQUE pour schema v11
  # Autres limites...
```

**Validation de la Correction**:
```bash
docker ps | grep loki  # Status "Up X seconds" 
docker logs w40k-loki --tail 5  # Pas d'erreur "empty ring"
curl http://localhost:3100/ready  # {"status":"ready"}
```

**Impact Business R√©solu**:
- ‚úÖ Collecte de logs op√©rationnelle  
- ‚úÖ Dashboards Grafana fonctionnels
- ‚úÖ Corr√©lation traces-logs restaur√©e

---

### ‚ùå Probl√®me #2: Grafana DNS Resolution Errors

**Sympt√¥mes Observ√©s**:
```
Get "http://loki:3100/loki/api/v1/query_range?...": 
dial tcp: lookup loki on 127.0.0.11:53: no such host
```
- Connection Loki failed dans Grafana datasources
- Dashboards logs compl√®tement vides
- Test connection √©choue syst√©matiquement

**Diagnostic Effectu√©**:
```bash
# Test r√©solution DNS depuis Grafana container
docker exec w40k-grafana nslookup loki  # NXDOMAIN

# V√©rification r√©seau Docker
docker network inspect w40kscoring_observability

# Test connectivit√© inter-containers  
docker exec w40k-grafana wget -qO- http://loki:3100/ready
```

**Cause Racine Identifi√©e**:
Probl√®me caus√© par le crash loop de Loki (Probl√®me #1). DNS interne Docker fonctionnel mais service Loki inaccessible.

**Solution Appliqu√©e**:
1. R√©solution du crash loop Loki (voir Probl√®me #1)
2. Red√©marrage ordonn√© des services:
```bash
docker-compose -f docker-compose.observability.yml restart loki
sleep 30  # Attendre stabilisation Loki
docker-compose -f docker-compose.observability.yml restart grafana
```

**Validation de la Correction**:
```bash
# Test DNS r√©solution
docker exec w40k-grafana nslookup loki  # Succ√®s

# Test connectivit√© API
curl -s "http://localhost:3100/loki/api/v1/query?query={job=\"docker\"}"  # Donn√©es retourn√©es

# V√©rification datasource Grafana
curl -s "http://admin:w40k-admin-2024@localhost:3000/api/datasources" | jq '.[] | select(.name=="Loki") | .basicAuth'
```

**Impact Business R√©solu**:
- ‚úÖ Dashboards logs op√©rationnels
- ‚úÖ Troubleshooting applicatif restaur√©  
- ‚úÖ Alerting bas√© sur logs fonctionnel

---

### ‚ùå Probl√®me #3: Logs Non Remont√©s dans Loki/Grafana

**Sympt√¥mes Observ√©s**:
- Dashboards logs compl√®tement vides
- Loki query: `{job="w40k-app"}` retourne `total_entries=0`
- Promtail fonctionne mais aucune donn√©e dans Loki
- Application logs visibles via `docker logs` mais pas dans Grafana

**Diagnostic Effectu√©**:
```bash
# Test direct API Loki
curl -s "http://localhost:3100/loki/api/v1/query_range?query={job=\"w40k-app\"}"

# V√©rification Promtail
docker logs w40k-promtail --tail 10

# Analyse configuration Promtail
docker exec w40k-promtail cat /etc/promtail/config.yml
```

**Cause Racine Identifi√©e**:
Application W40K tourne sur l'h√¥te via `npm run dev`, pas en container Docker. Promtail configur√© uniquement pour collecter logs Docker containers.

**Solutions Impl√©ment√©es**:

#### Solution A: Docker Service Discovery (Promtail)
```yaml
# observability/promtail.yml
scrape_configs:
  - job_name: w40k-docker-logs
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      - source_labels: [__meta_docker_container_name]
        target_label: container_name
```

#### Solution B: OpenTelemetry SDK Push (Application)
```typescript
// config/telemetry.ts - SDK int√©gration
const sdk = new NodeSDK({
  resource,
  instrumentations: [
    getNodeAutoInstrumentations({
      '@opentelemetry/instrumentation-pino': {
        enabled: true,
        logHook: (span, record) => {
          record['trace_id'] = span?.spanContext().traceId;
          record['span_id'] = span?.spanContext().spanId;
        },
      },
    }),
  ],
});
```

#### Solution C: Hybrid Collection Strategy
- **Pull-based**: Promtail pour containers Docker
- **Push-based**: OTLP SDK pour application host
- **Structured logs**: Corr√©lation avec trace IDs

**Validation de la Correction**:
```bash
# Test collection Docker logs
curl -s "http://localhost:3100/loki/api/v1/query?query={job=\"w40k-docker-logs\"}" | jq '.data.result | length'

# Test OTLP logs push
curl -s "http://localhost:3100/loki/api/v1/query?query={service_name=\"w40k-scoring\"}" | jq '.data.result | length'

# V√©rification corr√©lation traces
curl -s "http://localhost:3100/loki/api/v1/query?query={service_name=\"w40k-scoring\"}|=\"trace_id\"" | head -5
```

**Impact Business R√©solu**:
- ‚úÖ Visibilit√© compl√®te logs application
- ‚úÖ Debugging avec corr√©lation traces
- ‚úÖ Monitoring erreurs en temps r√©el
- ‚úÖ Hybrid strategy robuste et scalable

---

### ‚ùå Probl√®me #4: Code Quality Issues (ESLint Errors)

**Sympt√¥mes Observ√©s**:
- 18 erreurs ESLint bloquant le pipeline CI/CD
- Standards de code non respect√©s  
- Violations des r√®gles de qualit√© projet

**D√©tail des Erreurs Identifi√©es**:

#### telemetry_controller.ts (2 erreurs):
```typescript
// ‚ùå Erreur: Utilisation isNaN() deprecated
} else if (typeof value === 'number' && !isNaN(value)) {

// ‚ùå Erreur: Variable naming snake_case non conforme
const performance_level = value <= threshold.good ? 'good' : 'poor';
```

#### business_metrics_service.ts (14 erreurs):
```typescript
// ‚ùå Erreurs: parseInt/parseFloat globaux deprecated
return parseInt(count[0].$extras.total || '0');
return (parseFloat(result.avg_player) + parseFloat(result.avg_opponent)) / 2;
```

#### start/routes.ts (2 erreurs):
```typescript
// ‚ùå Erreurs: Acc√®s direct membres depuis await expressions
const BusinessMetricsService = (await import('#services/business_metrics_service')).default;
const SLOMetricsService = (await import('#services/slo_metrics_service')).default;
```

**Solutions Appliqu√©es**:

#### Corrections telemetry_controller.ts:
```typescript
// ‚úÖ Solution: Number.isNaN() et camelCase
} else if (typeof value === 'number' && !Number.isNaN(value)) {

const performanceLevel = value <= threshold.good ? 'good' : 'poor';
// Usage: performance_level: performanceLevel
```

#### Corrections business_metrics_service.ts:
```typescript
// ‚úÖ Solution: Number.parseInt() et Number.parseFloat()
return Number.parseInt(count[0].$extras.total || '0');
return (Number.parseFloat(result.avg_player) + Number.parseFloat(result.avg_opponent)) / 2;
```

#### Corrections start/routes.ts:
```typescript
// ‚úÖ Solution: S√©paration import et acc√®s propri√©t√©s
const BusinessMetricsServiceModule = await import('#services/business_metrics_service');
const BusinessMetricsService = BusinessMetricsServiceModule.default;
const SLOMetricsServiceModule = await import('#services/slo_metrics_service');
const SLOMetricsService = SLOMetricsServiceModule.default;
```

**Validation de la Correction**:
```bash
# Test linting complet
npm run lint  # Exit code 0, aucune erreur

# Validation TypeScript  
npm run typecheck  # Pas d'erreurs de type

# Tests de r√©gression
npm test  # 500 tests passed
```

**Impact Qualit√© R√©solu**:
- ‚úÖ Pipeline CI/CD d√©bloqu√©
- ‚úÖ Standards de code respect√©s
- ‚úÖ Conformit√© r√®gles ESLint projet
- ‚úÖ Maintenabilit√© code am√©lior√©e

---

## üîß Diagnostics par Service

### Prometheus

**Probl√®mes Courants**:
- Targets down/unreachable
- Scraping errors et timeouts  
- Storage full et retention issues

**Diagnostic Standard**:
```bash
# V√©rifier targets et leur √©tat
curl http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | {job: .labels.job, health: .health, error: .lastError}'

# Analyser erreurs scraping
docker logs w40k-prometheus --tail 20 | grep -i error

# V√©rifier usage disque  
docker exec w40k-prometheus df -h /prometheus
```

**Solutions Communes**:
```bash
# Target down ‚Üí V√©rifier app sur port 3333
curl http://localhost:3333/metrics

# Storage full ‚Üí Cleanup ou extension retention
# Modifier prometheus.yml: retention.time: 7d au lieu de 15d

# Scraping errors ‚Üí Corriger configuration
docker-compose -f docker-compose.observability.yml restart prometheus
```

### Loki

**Probl√®mes Courants**:
- Configuration schema incompatibilities
- Memory issues avec ingestion burst
- Compactor failures et corruption

**Diagnostic Standard**:
```bash
# V√©rifier configuration active
docker exec w40k-loki cat /etc/loki/local-config.yaml | grep -A5 limits_config

# Analyser compactor status
curl -s http://localhost:3100/compactor/ring | jq .

# Memory usage analysis
docker stats w40k-loki --no-stream
```

**Solutions Communes**:
```bash
# Schema compatibility
# Toujours utiliser: allow_structured_metadata: false pour v11

# Memory optimization
# Ajuster: ingestion_rate_mb et ingestion_burst_size_mb

# Compactor repair
docker-compose -f docker-compose.observability.yml restart loki
```

### Grafana

**Probl√®mes Courants**:
- Datasources unreachable apr√®s red√©marrage
- Dashboard import failures JSON invalid
- Permission errors sur volumes

**Diagnostic Standard**:
```bash
# Test datasources connectivity
curl -s "http://admin:w40k-admin-2024@localhost:3000/api/datasources" | jq '.[].url'

# V√©rifier dashboard provisioning
docker exec w40k-grafana ls -la /etc/grafana/provisioning/dashboards/

# Analyser permissions volumes
docker exec w40k-grafana ls -la /var/lib/grafana/
```

**Solutions Communes**:
```bash
# Datasource unreachable ‚Üí Test depuis container  
docker exec w40k-grafana wget -qO- http://prometheus:9090/-/ready

# Dashboard import failed ‚Üí Valider JSON syntax
jq . observability/grafana/dashboard-configs/w40k-*.json

# Permissions ‚Üí Fix ownership
docker exec w40k-grafana chown -R grafana:grafana /var/lib/grafana
```

### OpenTelemetry Collector  

**Probl√®mes Courants**:
- Receiver errors OTLP timeout
- Processor crashes memory limit
- Exporter timeouts vers backends

**Diagnostic Standard**:
```bash  
# V√©rifier configuration YAML
docker exec w40k-otel-collector cat /etc/otelcol-contrib/otel-collector.yml | yaml-validator

# Test endpoints OTLP  
curl -X POST http://localhost:4318/v1/traces -d '{}'

# Memory usage analysis
docker stats w40k-otel-collector --no-stream
```

**Solutions Communes**:
```bash
# Configuration invalid ‚Üí Fix YAML syntax
docker-compose -f docker-compose.observability.yml restart otel-collector

# Memory limit ‚Üí Ajuster memory_limiter processor
# memory_limiter.limit_mib: 512 (au lieu de 256)

# Network issues ‚Üí V√©rifier connectivity
docker exec w40k-otel-collector wget -qO- http://loki:3100/ready
```

---

## üìä Commandes de Diagnostic Essentielles

### V√©rification Globale Rapide

```bash
# Status tous services d'un coup
docker ps --format "table {{.Names}}\\t{{.Status}}\\t{{.Ports}}" | grep -E "(w40k|NAMES)"

# Health check automatis√© tous services  
for svc in grafana prometheus loki tempo otel-collector; do
  echo "=== $svc ==="
  docker logs w40k-$svc --tail 3 2>/dev/null || echo "Service not found"
  echo ""
done

# V√©rification r√©seau complete
docker network inspect w40kscoring_observability | jq '.[] | {Name: .Name, Containers: .Containers}'
```

### Tests de Connectivit√© Complets

```bash
# Endpoints de sant√© syst√®me
endpoints=(
  "localhost:3100/ready"           # Loki
  "localhost:9090/-/ready"         # Prometheus  
  "localhost:3000/api/health"      # Grafana
  "localhost:3200/ready"           # Tempo
  "localhost:13133/health"         # OTEL Collector
)

for endpoint in "${endpoints[@]}"; do
  if curl -s "http://$endpoint" > /dev/null; then
    echo "‚úÖ $endpoint - OK"
  else  
    echo "‚ùå $endpoint - FAILED"
  fi
done

# Test m√©triques application
curl -s http://localhost:3333/metrics | head -10 || echo "‚ùå App metrics unavailable"

# Test logs query fonctionnel
curl -G "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query={job=~".+"}' 2>/dev/null \
  | jq '.data.result | length' || echo "‚ùå Loki query failed"
```

### Debug Avanc√© Syst√®me

```bash
# Logs avec timestamps pour correlation
docker logs w40k-loki --timestamps --tail 50

# Stats containers avec utilisation historique
docker stats w40k-grafana w40k-loki w40k-prometheus --no-stream

# Inspect volumes et leur contenu  
docker volume ls | grep w40k
for vol in $(docker volume ls --format "{{.Name}}" | grep w40k); do
  echo "=== Volume: $vol ==="
  docker run --rm -v $vol:/data alpine du -sh /data
done

# Network inspection d√©taill√©e
docker network ls | grep observability
docker network inspect w40kscoring_observability | jq '.[] | .Containers | keys[]'
```

---

## üõ†Ô∏è Proc√©dures de R√©paration

### Red√©marrage Ordonn√© (Proc√©dure Recommand√©e)

```bash
#!/bin/bash
# restart-observability-stack.sh

echo "üîÑ Red√©marrage ordonn√© stack observabilit√©"
echo "========================================="

# 1. Arr√™t en ordre inverse d√©pendances
echo "‚èπÔ∏è Arr√™t des services..."
docker-compose -f docker-compose.observability.yml stop grafana
docker-compose -f docker-compose.observability.yml stop otel-collector  
docker-compose -f docker-compose.observability.yml stop prometheus
docker-compose -f docker-compose.observability.yml stop tempo
docker-compose -f docker-compose.observability.yml stop loki

# 2. Nettoyage optionnel containers
echo "üßπ Nettoyage containers..."
docker-compose -f docker-compose.observability.yml rm -f

# 3. V√©rification r√©seau
echo "üåê V√©rification r√©seau..." 
docker network prune -f

# 4. Red√©marrage ordonn√©
echo "üöÄ Red√©marrage des services..."
docker-compose -f docker-compose.observability.yml up -d loki
sleep 10
docker-compose -f docker-compose.observability.yml up -d tempo  
sleep 5
docker-compose -f docker-compose.observability.yml up -d prometheus
sleep 5  
docker-compose -f docker-compose.observability.yml up -d otel-collector
sleep 5
docker-compose -f docker-compose.observability.yml up -d grafana

# 5. Attendre stabilisation
echo "‚è≥ Attente stabilisation (60s)..."
sleep 60

# 6. V√©rification finale
echo "‚úÖ V√©rification status final..."
docker ps --format "table {{.Names}}\\t{{.Status}}" | grep w40k

# 7. Test connectivit√©  
echo "üîç Test connectivit√©..."
curl -s http://localhost:3000/api/health > /dev/null && echo "‚úÖ Grafana OK" || echo "‚ùå Grafana FAILED"
curl -s http://localhost:9090/-/ready > /dev/null && echo "‚úÖ Prometheus OK" || echo "‚ùå Prometheus FAILED"  
curl -s http://localhost:3100/ready > /dev/null && echo "‚úÖ Loki OK" || echo "‚ùå Loki FAILED"

echo "üéØ Red√©marrage termin√©"
```

### Reset Complet (Dernier Recours)

```bash
#!/bin/bash  
# emergency-reset.sh

echo "üö® RESET COMPLET - PERTE DE DONN√âES"
echo "===================================="
echo "‚ö†Ô∏è  Cette proc√©dure supprime TOUTES les donn√©es historiques"
echo "‚ö†Ô∏è  M√©triques, logs, traces seront perdus"
echo "‚ö†Ô∏è  Seules les configurations seront pr√©serv√©es"
echo ""

read -p "Confirmer le reset complet (tapez 'RESET'): " confirm
if [[ $confirm != "RESET" ]]; then
  echo "‚ùå Reset annul√©"
  exit 1
fi

echo "üõë Arr√™t complet stack..."
docker-compose -f docker-compose.observability.yml down

echo "üóëÔ∏è Suppression volumes donn√©es..."  
docker-compose -f docker-compose.observability.yml down -v

echo "üßπ Cleanup syst√®me..."
docker system prune -f
docker volume prune -f

echo "üìã Backup configurations..."
tar czf "observability-config-backup-$(date +%Y%m%d_%H%M).tar.gz" observability/

echo "üöÄ Red√©marrage √† z√©ro..."
docker-compose -f docker-compose.observability.yml up -d

echo "‚è≥ Attente initialisation (120s)..."
sleep 120

echo "‚úÖ Reset complet termin√©"
echo "üìä Acc√©der √† Grafana: http://localhost:3000 (admin/w40k-admin-2024)"
```

### R√©cup√©ration Configuration Selective

```bash
#!/bin/bash
# selective-recovery.sh

service=$1
if [[ -z $service ]]; then
  echo "Usage: $0 <service-name>"
  echo "Services: grafana, prometheus, loki, tempo, otel-collector"  
  exit 1
fi

echo "üîÑ R√©cup√©ration service: $service"

# Backup current config
echo "üíæ Backup configuration actuelle..."
cp -r "observability/" "observability-backup-$(date +%Y%m%d_%H%M)/"

# Stop specific service
echo "‚èπÔ∏è Arr√™t $service..."
docker-compose -f docker-compose.observability.yml stop $service

# Remove container (preserve volumes)
echo "üóëÔ∏è Suppression container..."  
docker-compose -f docker-compose.observability.yml rm -f $service

# Clean specific container data if needed
case $service in
  "grafana")
    docker volume rm w40kscoring_grafana-data 2>/dev/null || true
    ;;
  "prometheus")  
    docker volume rm w40kscoring_prometheus-data 2>/dev/null || true
    ;;
  "loki")
    docker volume rm w40kscoring_loki-data 2>/dev/null || true
    ;;
esac

# Restart service
echo "üöÄ Red√©marrage $service..."
docker-compose -f docker-compose.observability.yml up -d $service

echo "‚úÖ R√©cup√©ration $service termin√©e"
```

---

## üìà Monitoring de l'Infrastructure

### M√©triques Syst√®me Importantes

```promql
# Containers up/down
up{job=~".*w40k.*"}

# Usage ressources par container  
container_memory_usage_bytes{name=~"w40k-.*"} / 1024 / 1024 / 1024
container_cpu_usage_seconds_total{name=~"w40k-.*"}

# Sant√© Loki sp√©cifique
loki_ingester_chunks_created_total
loki_distributor_ingester_appends_total  
loki_distributor_ingester_append_failures_total

# Performance Prometheus
prometheus_rule_evaluation_duration_seconds
prometheus_tsdb_compactions_total
prometheus_tsdb_head_series

# Throughput OpenTelemetry Collector  
otelcol_processor_batch_send_size_count
otelcol_receiver_accepted_spans_total
otelcol_exporter_sent_spans_total
```

### Alertes Recommand√©es (Configuration Future)

```yaml
# alerts-observability.yml
groups:
  - name: w40k_observability_alerts
    rules:
      # Container health
      - alert: ObservabilityContainerDown
        expr: up{job=~".*w40k.*"} == 0  
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Service observabilit√© {{ $labels.job }} down"
          
      # High memory usage  
      - alert: ObservabilityHighMemoryUsage
        expr: container_memory_usage_bytes{name=~"w40k-.*"} / 1024 / 1024 / 1024 > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Service {{ $labels.name }} utilise >80% m√©moire"
          
      # Loki ingestion errors
      - alert: LokiIngestionErrors  
        expr: increase(loki_distributor_ingester_append_failures_total[5m]) > 10
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Loki erreurs ingestion √©lev√©es"
          
      # Prometheus target down
      - alert: PrometheusTargetDown
        expr: up{job="w40k-scoring-app"} == 0
        for: 1m  
        labels:
          severity: warning
        annotations:
          summary: "Application W40K indisponible pour scraping"
          
      # Grafana dashboard access
      - alert: GrafanaDashboardUnavailable
        expr: up{job="grafana"} == 0
        for: 3m
        labels: 
          severity: critical  
        annotations:
          summary: "Dashboards Grafana inaccessibles"
```

---

## üéØ Checklist de R√©solution

**Avant d'escalader un probl√®me, v√©rifier syst√©matiquement**:

### ‚úÖ Diagnostic Niveau 1 (2 min)
- [ ] Services status: `docker ps | grep w40k`  
- [ ] Logs r√©cents: `docker logs w40k-<service> --tail 20`
- [ ] Connectivit√© r√©seau: `curl http://localhost:<port>/health`
- [ ] Usage ressources: `docker stats --no-stream | grep w40k`

### ‚úÖ Diagnostic Niveau 2 (5 min)  
- [ ] Configuration validation: Syntax YAML correcte
- [ ] Volumes permissions: `docker exec <container> ls -la /data`
- [ ] R√©seau Docker: `docker network inspect w40kscoring_observability`
- [ ] Changements r√©cents: Git log, modifications config

### ‚úÖ Diagnostic Niveau 3 (10 min)
- [ ] Tests connectivit√© inter-services depuis containers
- [ ] Analyse m√©moire/CPU detailed avec historique
- [ ] V√©rification int√©grit√© donn√©es volumes  
- [ ] Comparaison avec √©tat fonctionnel ant√©rieur (backups)

### ‚úÖ Solutions Appliqu√©es Syst√®me
- [ ] Probl√®me Loki crash loop: `allow_structured_metadata: false`
- [ ] DNS resolution: D√©pendance Loki fix√©e ‚Üí restart Grafana  
- [ ] Log collection: Hybrid strategy (Pull + Push OTLP)
- [ ] Code quality: 18 erreurs ESLint corrig√©es

### üìã Informations pour Support Avanc√©

**En cas d'escalation, fournir**:
```bash
# Collecte informations compl√®te
./collect-diagnostic-info.sh > diagnostic-$(date +%Y%m%d_%H%M).txt

# Contient:
# - Status tous containers
# - Logs 100 derni√®res lignes par service  
# - Configuration files checksums
# - Resource usage historique
# - Network topology 
# - Recent changes timeline
```

---

**Guide de troubleshooting valid√© avec r√©solutions prouv√©es** ‚úÖ  
**Proc√©dures test√©es en environnement de d√©veloppement** üîß  
**Solutions applicables en production** üöÄ